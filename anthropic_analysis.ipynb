{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f691a21",
   "metadata": {},
   "source": [
    "# Anthropic Interviewer Analysis: Workforce Insights\n",
    "\n",
    "## Executive Summary\n",
    "This notebook presents a comprehensive analysis of how professionals interact with AI assistants, based on the **Anthropic Interviewer** dataset.\n",
    "\n",
    "**Key Findings:**\n",
    "*   **Trust is the Barrier:** 8.64% of interactions explicitly mention errors or hallucinations, identifying \"Verification Friction\" as a core user pain point.\n",
    "*   **The \"Symbiotic\" User Cluster:** We identified a specific group of users (Cluster 1) who engage in deep, iterative refinement with high technical language, contrasting with \"Efficiency Seekers\" who use AI for quick delegation.\n",
    "*   **Frustration Context:** Semantic network analysis reveals that frustration is often linked to specific context failures rather than general incompetence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72791a05",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "To derive these insights, we processed the raw interview transcripts through the following pipeline:\n",
    "\n",
    "1.  **Data Loading**: Fetched the `workforce` split from Hugging Face.\n",
    "2.  **Preprocessing & Segmentation**:\n",
    "    *   Used **Regex** to split transcripts into `User` and `Assistant` turns.\n",
    "    *   Filtered out metadata headers to focus purely on user intent.\n",
    "3.  **Advanced Analysis**:\n",
    "    *   **TF-IDF**: For topic modeling.\n",
    "    *   **K-Means Clustering**: To create the *AI Maturity Matrix* based on interaction features (verbosity, complexity, refinement count).\n",
    "    *   **NetworkX**: To build semantic graphs of co-occurring terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12661181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Analysis Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f1f45",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "We start by loading the data and defining our cleaning/segmentation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0debf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\\n",
    "def segment_dialogue(transcript):\n",
    "    parts = re.split(r'(User:|Assistant:)', transcript)\n",
    "    turns = []\n",
    "    current_role = None\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if part == \"User:\": current_role = \"user\"\n",
    "        elif part == \"Assistant:\": current_role = \"assistant\"\n",
    "        elif part and current_role:\n",
    "            turns.append({'role': current_role, 'content': part})\n",
    "    return turns\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"Anthropic/AnthropicInterviewer\", split='workforce')\n",
    "df_raw = dataset.to_pandas()\n",
    "\n",
    "# Apply Segmentation\n",
    "all_turns = []\n",
    "for _, row in df_raw.iterrows():\n",
    "    turns = segment_dialogue(row['text'])\n",
    "    for turn in turns:\n",
    "        turn['transcript_id'] = row['transcript_id']\n",
    "        all_turns.append(turn)\n",
    "\n",
    "df_turns = pd.DataFrame(all_turns)\n",
    "print(f\"Total Turns Extracted: {len(df_turns)}\")\n",
    "print(df_turns['role'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7798c",
   "metadata": {},
   "source": [
    "## The Analysis: AI Maturity Matrix (Clustering)\n",
    "\n",
    "We define **AI Maturity** not just by frequency of use, but by the depth of interaction. We engineered features like **Verbosity** (Avg Length), **Complexity** (Vocabulary richness), and **Refinement Count** (how often they correct the AI).\n",
    "\n",
    "We use **K-Means Clustering** to segment users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_maturity_clusters(df_turns, n_clusters=3):\n",
    "    user_groups = df_turns[df_turns['role'] == 'user'].groupby('transcript_id')\n",
    "    user_features = []\n",
    "    user_ids = []\n",
    "    \n",
    "    for uid, group in user_groups:\n",
    "        full_text = \" \".join(group['content'].astype(str))\n",
    "        avg_len = group['content'].str.len().mean()\n",
    "        words = full_text.split()\n",
    "        complexity = len(set(words)) / len(words) if words else 0\n",
    "        refinement_keywords = ['change', 'wrong', 'mistake', 'revise', 'no', 'update']\n",
    "        refinement_score = sum(full_text.lower().count(k) for k in refinement_keywords)\n",
    "        tech_keywords = ['code', 'python', 'sql', 'data', 'function', 'api']\n",
    "        tech_score = sum(full_text.lower().count(k) for k in tech_keywords)\n",
    "        \n",
    "        user_features.append([avg_len, complexity, refinement_score, tech_score])\n",
    "        user_ids.append(uid)\n",
    "        \n",
    "    X = np.array(user_features)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    coords = pca.fit_transform(X)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'cluster': labels, 'x': coords[:, 0], 'y': coords[:, 1],\n",
    "        'avg_len': X[:, 0], 'complexity': X[:, 1], 'refinement': X[:, 2]\n",
    "    })\n",
    "\n",
    "print(\"Running Clustering...\")\n",
    "cluster_df = analyze_maturity_clusters(df_turns)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=cluster_df, x='x', y='y', hue='cluster', palette='viridis', s=100)\n",
    "plt.title(\"AI Maturity Clusters: Segmentation of User Behavior\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d35fb",
   "metadata": {},
   "source": [
    "## The Analysis: Semantic Network (Why \"Frustrated\"?)\n",
    "\n",
    "We explore the \"root causes\" of user frustration by building a co-occurrence graph around the word **\"frustrated\"**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def analyze_semantic_network(df_turns, target_word=\"frustrated\", window_size=5):\n",
    "    user_turns = df_turns[df_turns['role'] == 'user']['content'].tolist()\n",
    "    texts = [re.sub(r'[^\\w\\s]', '', t.lower()).split() for t in user_turns]\n",
    "    \n",
    "    co_occurrence = Counter()\n",
    "    for tokens in texts:\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token == target_word: # Optimization: only look around target\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(len(tokens), i + window_size + 1)\n",
    "                for neighbor in tokens[start:i] + tokens[i+1:end]:\n",
    "                    # FILTER: Ignore stop words and short words\n",
    "                    if neighbor not in ENGLISH_STOP_WORDS and len(neighbor) > 2:\n",
    "                        co_occurrence[tuple(sorted((token, neighbor)))] += 1\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for (w1, w2), count in co_occurrence.most_common(30):\n",
    "        G.add_edge(w1, w2, weight=count)\n",
    "    return G\n",
    "\n",
    "G = analyze_semantic_network(df_turns, \"frustrated\")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Use Shell Layout for symmetry (Hub and Spoke)\n",
    "others = [n for n in G.nodes() if n != \"frustrated\"]\n",
    "if \"frustrated\" in G:\n",
    "    pos = nx.shell_layout(G, nlist=[[\"frustrated\"], others])\n",
    "else:\n",
    "    pos = nx.spring_layout(G, k=0.3)\n",
    "\n",
    "# Dynamic Edge Widths\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] for u, v in edges]\n",
    "max_weight = max(weights) if weights else 1\n",
    "widths = [(w / max_weight) * 5 for w in weights]\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='#ffcccb')\n",
    "nx.draw_networkx_edges(G, pos, width=widths, alpha=0.5)\n",
    "nx.draw_networkx_labels(G, pos, font_weight='bold')\n",
    "\n",
    "plt.title(\"Semantic Context of 'Frustrated' (Weighted & Symmetrical)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f65d95",
   "metadata": {},
   "source": [
    "## Actionable Insight & Recommendations\n",
    "\n",
    "Based on the data, we recommend the following strategic actions:\n",
    "\n",
    "1.  **Context-First Training:**\n",
    "    *   *Insight:* Frustration is semantically linked to missed context.\n",
    "    *   *Action:* Organizations should train employees on \"Context Loading\" prompting techniques (providing background *before* the request) to reduce the 8.64% error rate.\n",
    "\n",
    "2.  **Bridge the Gap for \"Efficiency Seekers\":**\n",
    "    *   *Insight:* Cluster 2 users have very short, transactional interactions and miss out on iterative refinement.\n",
    "    *   *Action:* Introduce \"Refinement Templates\" that encourage these users to ask for revisions (e.g., \"Critique this\", \"Make it more concise\"), moving them towards the \"Symbiotic\" model.\n",
    "\n",
    "3.  **Formalize the \"Human Review\":**\n",
    "    *   *Insight:* Trust remains a barrier.\n",
    "    *   *Action:* Instead of hoping for perfect AI accuracy, integrate a formal \"AI Review Step\" into standard operating procedures (SOPs), acknowledging that AI is a generator, not a finalizer.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
